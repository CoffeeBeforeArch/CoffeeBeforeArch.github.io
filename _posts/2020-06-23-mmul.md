---
layout: default
title: Optimizing Matrix Multiplication
---

# Optimizing Matrix Multiplication

Matrix multiplication is an incredibly common operation across numerous domains. It is also known as being "embarrissingly parallel". As such, one common optimization is parallelization across threads on a multi-core CPU or GPU. However, parallelization is not a panacea. Poorly parallelized code may provide minimal speedups (if any). In this blog post, we'll be comparing a few different implementations of matrix multiplication, and show how we can get significant performance improvement from both restructuring access patterns and parallelization.

## Links
- [These Benchmarks on GitHub](https://github.com/CoffeeBeforeArch/gemm)
- [My YouTube Channel](https://www.youtube.com/channel/UCsi5-meDM5Q5NE93n_Ya7GA?view_as=subscriber)
- [My GitHub Account](https://github.com/CoffeeBeforeArch)
- My Email: CoffeeBeforeArch@gmail.com

## My Testing Environment

The results in this blog post were collected on my home desktop running Ubuntu 20.04, with an 8 core Intel Core i7-9700 processor. All benchmarks were written using [Google Benchmark](https://github.com/google/benchmark), and hotspot/performance counter information was collected using the [perf Linux profiler](https://perf.wiki.kernel.org/index.php/Main_Page).

The benchmarks can be found in the [gemm repo](https://github.com/CoffeeBeforeArch/gemm) on my GitHub page. There is a simple `Makefile` in the src directory that can be used to build the benchmarks. The benchmarks are compiled using the following flags:

```
CXX_FLAGS = -O3 -march=native -mtune=native -flto -fuse-linker-plugin --std=c++2a
```

It also requires you to use the linker flags `-lbenchmark` and `lpthread` (both of which are required for Google Benchmark).

## Matrix Multiplication - Baseline

Let's look at an implementation of matrix multiplication that will likely be familiar to many of you.

```cpp
void serial_gemm(const double *A, const double *B, double *C, std::size_t N) {
  // For each row...
  for (std::size_t row = 0; row < N; row++)
    // For each col...
    for (std::size_t col = 0; col < N; col++)
      // For each element in the row/col pair...
      for (std::size_t idx = 0; idx < N; idx++)
        // Accumulate the partial results
        C[row * N + col] += A[row * N + idx] * B[idx * N + col];
}
```

Let's break down this function loop-by-loop to understand how we are traversing the A, B, and C matrices (something we will do for each of these examples).

The outermost-loop of the function selects the row of the element we are going to solve for.

![serial row](/assets/mmul/serial_row.png)

The next loop then selects the column of the element we are solving for.

![serial col](/assets/mmul/serial_col.png)

The combination of the row and column index gives us a coordinate of a single element in the output matrix C to solve for. The final, inner-most loop performs this computation by doing the dot-product of one Row of the A matrix, and one column of the B matrix.

![serial dot product](/assets/mmul/serial_dot_product.png)

Computationally, how much work are we doing? Well, for the square matrices of dimension NxN we are working with, we perform N multiplies and adds for the dot product for each element. This results in N^3 FMA (fused multiply-add operations).

Now that we understand the computational complexity of this algorithm, let's take a look at some initial performance numbers. Let's test our serial gemm function for square matrix sizes of 2^8, 2^9, and 2^10. Here are the results.

```
-------------------------------------------------------------------------
Benchmark                               Time             CPU   Iterations
-------------------------------------------------------------------------
serial_gemm_bench_power_two/8        20.6 ms         20.6 ms           68
serial_gemm_bench_power_two/9         169 ms          169 ms            8
serial_gemm_bench_power_two/10       1439 ms         1439 ms            1
```

Pretty slow for the 2^10 case. However, we are running into a rather nasty performance corner case using power-of-two dimension matrices. Each access we make to our B matrix is 2^N elements apart. Having an access pattern with kind of stride can lead to many conflict misses, as each cache line will be mapped to a small subset of cache sets (or even a single cache set).

![power of two cache](/assets/mmul/power_of_two_cache.png)

While our cache sizes may be relatively large (at least compared to the size of our matrices), we may not be able to make effective use of them if we have such an access pattern. Let's try slightly increasing the size of our matrix, and re-collect the performance numbers. We'll use 2^8 + 16, 2^9 + 16, and 2^10 + 16. Here are the performance numbers:

```
---------------------------------------------------------------
Benchmark                     Time             CPU   Iterations
---------------------------------------------------------------
serial_gemm_bench/8        16.2 ms         16.2 ms           85
serial_gemm_bench/9         126 ms          126 ms           11
serial_gemm_bench/10       1067 ms         1067 ms            1
```

Despite doing more work, our performance improves (and by a fairly large margin!). We'll use these augmented sizes (2^8 + 16, 2^9 + 16, and 2^10 + 16) for the rest of our experiments.

## Matrix Multiplication - Parallel

Now that we have our baseline performance numbers, let's see if we can get some extra performance out of parallelization. Here is how my implementation looks.

```cpp
// Parallel implementation
void parallel_gemm(const double *A, const double *B, double *C, std::size_t N,
                   std::size_t start_row, std::size_t end_row) {
  // For each row assigned to this thread...
  for (std::size_t row = start_row; row < end_row; row++)
    // For each column...
    for (std::size_t col = 0; col < N; col++)
      // For each element in the row-col pair...
      for (std::size_t idx = 0; idx < N; idx++)
        // Accumulate the partial results
        C[row * N + col] += A[row * N + idx] + B[idx * N + col];
```

We can divide up the work by dividing the rows of the output matrix to process. The only change I've made to the original serial code is passing in the start row and end row to the function. Here is figure of how we have divided up the work.

![baseline parallel](/assets/mmul/baseline_parallel.png)

Assuming we can divide our matrix evenly by the number of threads we launch (assumed to be true), each thread performs the same ammount of work. Let's check out how our performance improved.

```
---------------------------------------------------------------------------
Benchmark                                 Time             CPU   Iterations
---------------------------------------------------------------------------
parallel_gemm_bench/8/real_time        2.46 ms         2.42 ms          579
parallel_gemm_bench/9/real_time        17.5 ms         17.4 ms           78
parallel_gemm_bench/10/real_time        152 ms          152 ms            9
```

A huge improvement! For our largest matrix size (2^10 + 16), we've improved execution time from ~1000ms to ~150ms. That's a huge improvement for minimal changes to our original function (plus a few extra lines for spawning/joining threads). However, we're far from done.

## Matrix Multiplication - Blocked

Let's see if we can be a little more clever about the order in which we process elements. Specifically, let's begin to focus on exploiting locality in the B matrix. In our serial benchmark, we access one element from each row of the B matrix (from Row 0 -> N-1). However, that's not what our processor is doing under the hood. When we access a single element from B, our processor loads the entire cache line that element is sitting on into the cache. Most processors have a 64B cache line, which can fit at most 16 ints/float, or 8 doubles.

If we only access a single element from each row of the B matrix at a time, we're wasting the other elements on that cache line it is sitting on. By the time we get back around to accessing the other elements, they may have already been replaced in the cache. Instead of processing a single element at a time, let's process an entire block of elements at a time to try improve locality (this optimization is often called blocking or tiling). Here is my code.

```cpp
// Blocked serial implementation
void blocked_gemm(const double *A, const double *B, double *C, std::size_t N) {
  // For each row...
  for (std::size_t row = 0; row < N; row++)
    // For each block in the row...
    // Solve for 16 elements at a time
    for (std::size_t block = 0; block < N; block += 16)
      // For each chunk of A/B for this block
      for (std::size_t chunk = 0; chunk < N; chunk += 16)
        // For each row in the chunk
        for (std::size_t sub_chunk = 0; sub_chunk < 16; sub_chunk++)
          // Go through all the elements in the sub chunk
          for (std::size_t idx = 0; idx < 16; idx++)
            C[row * N + block + idx] +=
                A[row * N + chunk + sub_chunk] *
                B[chunk * N + sub_chunk * N + block + idx];
}
```

All we've done is added a few extra loops to handle the blocking/tiling. Let's take a look at how we've decomposed the computation into blocks.

We can start with the outer-most loop. We're processing output elements of the C matrix one row at a time (the same as our serial implementation).

![blocked row](/assets/mmul/blocked_row.png)

However, our next loop is where we change things up. Instead of processing elements one column at a time, we're going to process elements a block at a time. This will allow us to re-use elements loaded in from the B matrix.

![blocked block](/asserts/mmul/blocked_block.png)

For each block of elements we are solving for, our next loop processes elements of A and B matrices one chunk at a time. Since we're not processing the entirety of the matrices at once, the pieces are more likely to fit into our caches.

![blocked chunk](/assets/mmul/blocked_chunk.png)

The next loop is for processing elements from each chunk (we'll call it a sub chunk). We'll go through each row of the chunk of the B matrix, and each element from the single chunk of the A matrix.

![blocked chunk](/assets/mmul/blocked_sub_chunk.png)

The final loop is for actually performing the computation (yay, we made it!). In this loop, we multiple the elements of the B matrix by one of the elements loaded in by the A matrix, and storing the partial results in the block of elements of the C matrix.

![blocked dot product](/assets/mmul/blocked_dot_product.png).

Now that we have some understanding of what this loop is doing, let's look at the performance results.

```
-----------------------------------------------------------------------------------
Benchmark                                         Time             CPU   Iterations
-----------------------------------------------------------------------------------
blocked_gemm_bench/8                           2.67 ms         2.67 ms          535
blocked_gemm_bench/9                           22.0 ms         21.9 ms           63
blocked_gemm_bench/10                           255 ms          255 ms            5
```

Impressive results! We're almost as fast as our parallelized version with just a single thread (~150ms vs ~250ms).

However, we're not quite done with our blocked version yet. Our performance is based on our assumption about exploiting locality. To help out with this, we can use something like `aligned_alloc` have our matrix aligned to the start of a cache line. While this isn't guaranteed to help performance, it can in cases where the alignment is important. Here's our performance results when using `aligned_alloc` instead of `new` for our matrices.

```
------------------------------------------------------------------------
Benchmark                              Time             CPU   Iterations
------------------------------------------------------------------------
blocked_aligned_gemm_bench/8        2.51 ms         2.51 ms          552
blocked_aligned_gemm_bench/9        19.6 ms         19.6 ms           71
blocked_aligned_gemm_bench/10        256 ms          256 ms            5
```

Not a huge performance improvement, but it did help slightly with our smaller two matrix sizes.



## Blocked Column

One way of blocking is across a row of the C matrix. This is what we just did. However, this does a lot of potentially wasted work. Whenever we move to a new block, we load in a completely new 16 columns of the B matrix, and re-use a single row of the A matrix. This seems like a waste. Instead, let's re-use the Columns of B. Here is my implementation of a blocked column algorithm:

```cpp
// Blocked serial implementation
void blocked_column_gemm(const double *A, const double *B, double *C,
                         std::size_t N) {
  // For each chunk of columns
  for (std::size_t col_chunk = 0; col_chunk < N; col_chunk += 16)
    // For each row in that chunk of columns...
    for (std::size_t row = 0; row < N; row++)
      // For each block of elements in this row of this column chunk
      // Solve for 16 elements at a time
      for (std::size_t tile = 0; tile < N; tile += 16)
        // For each row in the tile
        for (std::size_t tile_row = 0; tile_row < 16; tile_row++)
          // Solve for each element in this tile row
          for (std::size_t idx = 0; idx < 16; idx++)
            C[row * N + col_chunk + idx] +=
                A[row * N + tile + tile_row] *
                B[tile * N + tile_row * N + col_chunk + idx];
}
```

The outermost loop select which columns of elements we are going to solve from the C matrix.

![blocked column col chunk](/assets/mmul/blocked_column_col_chunk.png)

Then we go through all the rows inside of that column chunk. All the elements of these rows will re-use the same columns from the B matrix.

![blocked column row](/assets/mmul/blocked_column_row.png)

From there, the process is exactly the same as our original blocked algorithm. We start by only processing one tile of elements at a time from matrices A and B.

![blocked column tile](/assets/mmul/blocked_column_tile.png)

For each Tile, we then have to go through all the rows of the B tile, and the coulmns of the A tile.

![blocked column tile row](/assets/mmul/blocked_column_tile_row.png)

Finally, we multiply each element in the row of the B matrix tile, with a single element from the A matrix tile, and accumulate the partial results into the C matrix.

Now that we understand the algorithm let's look at the performance numbers:

```
-------------------------------------------------------------------------------
Benchmark                                     Time             CPU   Iterations
-------------------------------------------------------------------------------
blocked_column_aligned_gemm_bench/8        1.80 ms         1.80 ms          772
blocked_column_aligned_gemm_bench/9        14.5 ms         14.5 ms           95
blocked_column_aligned_gemm_bench/10        138 ms          138 ms           10
```

We now have a single-threaded version that is faster than our parallel implementation for all cases!

## Parallel Blocked Column

Let's parallelize our blocked column algorithm. However, we have to decompose our problem slightly differently. Because our algorithm is based around blocks of columns we have less flexibility in how we divide up our work. This is because the number of columns we must give each thread must be a multiple of 16 (because the outer loop increments by 16). However, we can get around this fairly easily by having each thread work on only 16 columns at a time, and come back for more when they are done.

```cpp
 // Blocked serial implementation
 void blocked_column_parallel_atomic_gemm(const double *A, const double *B,
                                          double *C, std::size_t N,
                                          std::atomic<uint64_t> &pos) {
   for (auto col_chunk = pos.fetch_add(16); col_chunk < N;
        col_chunk = pos.fetch_add(16))
     // For each row in that chunk of columns...
     for (std::size_t row = 0; row < N; row++)
       // For each block of elements in this row of this column chunk
       // Solve for 16 elements at a time
       for (std::size_t tile = 0; tile < N; tile += 16)
         // For each row in the tile
         for (std::size_t tile_row = 0; tile_row < 16; tile_row++)
           // Solve for each element in this tile row
           for (std::size_t idx = 0; idx < 16; idx++)
             C[row * N + col_chunk + idx] +=
                 A[row * N + tile + tile_row] *
                 B[tile * N + tile_row * N + col_chunk + idx];
 }
```

Each iteration of the outer loop, the starting column number is given by the call to `pos.fetch_add(16)`. This increments the current global column number by 16, and return the previous value. While each thread pays the price for finding what chunk of columns they are working on next, this is is relatively small, as it happens only once in the outermost loop. Furthermore, we don't have to handle any nasty corner cases in our code.

![blocked column parallel](/assets/mmul/blocked_column_parallel.png)

Here are the performance results:

```
-------------------------------------------------------------------------------------------------
Benchmark                                                       Time             CPU   Iterations
-------------------------------------------------------------------------------------------------
parallel_blocked_column_atomic_gemm_bench/8/real_time       0.614 ms        0.467 ms         2307
parallel_blocked_column_atomic_gemm_bench/9/real_time        3.62 ms         2.97 ms          382
parallel_blocked_column_atomic_gemm_bench/10/real_time       30.0 ms         29.3 ms           46
```

A huge improvement over both our baseline parallel implementation and the serial blocked column implementation.

## Concluding Remarks

Performance does not require everything to be hand-written in assembly, nor does it always require some exceeedingly complex algorithm. Paying attention to what data you are accessing, if/when you are re-accessing it, and finding ways to make best use of this locality can lead to significant performance improvements.

We started with a matrix multiplication algorith

As always, feel free to contact me with questions.

Cheers,

--Nick

## Links

- [My YouTube Channel](https://www.youtube.com/channel/UCsi5-meDM5Q5NE93n_Ya7GA?view_as=subscriber)
- [My GitHub Account](https://github.com/CoffeeBeforeArch)
- My Email: CoffeeBeforeArch@gmail.com
