---
layout: default
title: Spinlocks - Part 3
---

# Spinlocks - Part 3

In the [last blog post](https://coffeebeforearch.github.io/2020/11/07/spinlocks-2.html), we used an optimization called locally spinning to alleviate the high number of coherence misses in our initial `naive` spinlock implementation. However, at the end of that blog post, I mentioned that we've traded out our constant contention for our cache lines with bursty contention.

In this blog post, we'll spend some time understanding this bursty contention and try and address it using a spinlock optimization called active backoff.

### Link to the source code

- [Source Code: ](https://github.com/CoffeeBeforeArch/spinlocks)
- [My YouTube Channel: ](https://www.youtube.com/channel/UCsi5-meDM5Q5NE93n_Ya7GA?view_as=subscriber)
- [My GitHub Account: ](https://github.com/CoffeeBeforeArch)
- My Email: CoffeeBeforeArch@gmail.com

## Our Benchmark

We will evaluate our spinlock implementations using [Google Benchmark](https://github.com/google/benchmark). The structure of the benchmark can be found below:

```cpp
 // Small Benchmark
 static void bench(benchmark::State &s) {
   // Sweep over a range of threads
   auto num_threads = s.range(0);
 
   // Value we will increment
   std::int64_t val = 0;
 
   // Allocate a vector of threads
   std::vector<std::thread> threads;
   threads.reserve(num_threads);
 
   Spinlock sl;
 
   // Timing loop
   for (auto _ : s) {
     for (auto i = 0u; i < num_threads; i++) {
       threads.emplace_back([&] { inc(sl, val); });
     }
     // Join threads
     for (auto &thread : threads) thread.join();
     threads.clear();
   }
 }
 BENCHMARK(bench)
     ->RangeMultiplier(2)
     ->Range(1, std::thread::hardware_concurrency())
     ->UseRealTime()
     ->Unit(benchmark::kMillisecond);
```

We'll be evaluating our benchmark for a power-of-two number of threads (1, 2, 4, and 8 on my machine), where each thread runs a function called `inc`. This function can be found below:

```cpp
 // Increment val once each time the lock is acquired
 void inc(Spinlock &s, std::int64_t &val) {
   for (int i = 0; i < 100000; i++) {
     s.lock();
     val++;
     s.unlock();
   }
 }
```

For 100k iterations, our threads try and lock our spinlock, increment a shared value, then release the lock. This gives us a heavy contention scenario in which to test our spinlock.

## A Spinlock with Active Backoff

Before we jump into our active backoff optimization implementation, let's try and understand where we have bursty contention in our spinlock implementation.

### Bursty Contention

Our bursty contention occurs when the thread currently holding the lock releases the lock. Let's take another look at our lock method to understand this:

```cpp
   // Lock the spinlock
   void lock() {
     // Keep trying until we get the lock
     while (1) {
       // Try and grab the lock
       if (!locked.exchange(true)) return;
 
       // Read the spinlock state
       while (locked.load())
         ;
     }
   }
```
 
When the lock is released, all threads spinning in the second `while` loop (our locally spinning optimization) read that the lock is released, and race to the atomic exchange to try and grab the lock. However, only one thread out of the potentially many that try for the lock will get the lock. All the other threads will then wastefully perform their atomic exchange, fail to get the lock, and go back to reading the lock state, waiting for it to become free again.

To solve this, we need to limit the number of threads that can break out of our waiting loop when the lock is freed. We can do this by adding backoff to this loop.

### The Active Backoff Optimization

To keep our threads from immediately breaking out of the second `while` loop, we will be adding a small delay between each read of the spinlock state. Here is our modified `lock` method:

```cpp
   // Locking mechanism
   void lock() {
     while (1) {
       // Try and grab the lock
       if (!locked.exchange(true)) return;
 
       // Wait for the lock to be free
       do {
         // Pause for some number of iterations
         for (volatile int i = 0; i < 150; i += 1)
           ;
         // Read the lock state
       } while (locked.load());
     }
   }
```

Our `lock` method is almost identical to our spinning locally implemention, with the exception of our spinning locally optimization loop. Instead of reading the lock state as fast as possible, we'll insert a configurable delay using a dummy `for` loop.

But what does inserting a delay here accomplish? If threads read the state of the lock as fast as possible, it's likely that almost all the threads will break out at and compete for the lock at the same time. By adding a delay in this loop, we increase the probability that some threads will get caught up executing the delay instructions from our dummy `for` loop, while others (but not _all_ the threads) break out at try and get the lock. 

Here is our full spinlock implementation:

```cpp
// Simple Spinlock
// Lock now performs local spinning
// Lock now does backoff
class Spinlock {
 private:
  // Lock is just an atomic bool
  std::atomic<bool> locked{false};

 public:
  // Locking mechanism
  void lock() {
    // Keep trying
    while (1) {
      // Try and grab the lock
      // Exit if we got the lock
      if (!locked.exchange(true)) return;

      // If we didn't get the lock, just read the value which gets cached
      // locally. This leads to less traffic.
      // Each iteration we can also call pause to limit the number of writes.
      // How many times you should pause each time should be experimentally
      // determined
      do {
        // Pause for some number of iterations
        for (volatile int i = 0; i < 150; i += 1)
          ;
      } while (locked.load());
    }
  }
```

### Low-Level Assembly

Let's take a look at how our low-level assembly for our benchmark looks with this new optimization:

```assembly
  0.42 │20:┌─→mov     %edi,%ecx              
 17.29 │   │  xchg    %cl,(%rax)             
  0.01 │   │  test    %cl,%cl                
  0.01 │   │↓ je      67                     
  0.09 │   │  nop                            
  0.35 │30:│  movl    $0x0,-0x4(%rsp)        
  0.07 │   │  mov     -0x4(%rsp),%ecx        
  0.02 │   │  cmp     $0x95,%ecx             
       │   │↓ jg      5e                     
  0.06 │   │  nop                            
  1.77 │48:│  mov     -0x4(%rsp),%ecx        
  1.44 │   │  inc     %ecx                   
 15.57 │   │  mov     %ecx,-0x4(%rsp)        
 21.50 │   │  mov     -0x4(%rsp),%ecx        
  0.07 │   │  cmp     $0x95,%ecx             
  8.49 │   │↑ jle     48                     
 13.96 │5e:│  movzbl  (%rax),%ecx            
  0.04 │   │  test    %cl,%cl                
  0.02 │   │↑ jne     30                     
  0.35 │   │↑ jmp     20                     
  2.54 │67:│  incq    (%rsi)                 
 15.93 │   │  xchg    %cl,(%rax)             
       │   ├──dec     %edx                   
  0.00 │   └──jne     20                     
```

Our assembly is very similar to our spinning locally implementation, with the exception of the extra instructions for our delay `for` loop. This loop is set up at label `30:`, and begins at label `48:`.

### Performance

Here are the end-to-end performance numbers for 1, 2, 4, and 8 threads:

```
---------------------------------------------------------------------
Benchmark                           Time             CPU   Iterations
---------------------------------------------------------------------
active_backoff/1/real_time       1.05 ms        0.054 ms          679
active_backoff/2/real_time       2.60 ms        0.081 ms          281
active_backoff/4/real_time       9.49 ms        0.121 ms           75
active_backoff/8/real_time       50.5 ms        0.287 ms           14
```

And here are the performance numbers from our previous (`spin_locally`) spinlock implementation:

```txt
-------------------------------------------------------------------
Benchmark                         Time             CPU   Iterations
-------------------------------------------------------------------
spin_locally/1/real_time       1.01 ms        0.046 ms          698
spin_locally/2/real_time       10.4 ms        0.070 ms           68
spin_locally/4/real_time       28.9 ms        0.123 ms           19
spin_locally/8/real_time       91.4 ms        0.148 ms            8
```

Another huge improvement! Compared to our spinlock with only the locally spinning optimization, our end-to-end execution time went down from 10.4ms to 2.60ms for 2 threads, 28.9ms to 9.49ms for 4 threads, and 91.4ms to 50.5ms for 8 threads. 

While this is a great improvement in performance, we're wasting power executing all the instructions for the `for` loop that generates our delay. We can look at addressing this using a passive form of backoff.

## Final Thoughts

A relatively minor change to our code (replacing reads with writes) lead to a huge improvement in performance. However, there is still plenty of room for improvement in our implementation, especially centering around how fast our threads read the spinlock state.

Thanks for reading,

--Nick

### Link to the source code

- [Source Code: ](https://github.com/CoffeeBeforeArch/spinlocks)
- [My YouTube Channel: ](https://www.youtube.com/channel/UCsi5-meDM5Q5NE93n_Ya7GA?view_as=subscriber)
- [My GitHub Account: ](https://github.com/CoffeeBeforeArch)
- My Email: CoffeeBeforeArch@gmail.com

## A Spinlock with Passive Backoff

While everyone cares about performance (to some degree), an increasing number of people also care about power-consumption (especially in massive data-centers). One way we can intuitively save on power is by improving performance so that our application finishes faster (the processor is active for a shorter amount of time). However, there are ways we can improve power-consumption for applications that run for (approximately) the same length of time.

For spin-wait loops like our spinlock, we can use the `_mm_pause()` intrinsic for this exact purpose.

### The Pause Intrinsic

[Here](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_pause&expand=4141) is the `_mm_pause` intrinsic from the intel Intrinsics Guide. If we look at the description, it says:

```txt
Provide a hint to the processor that the code sequence is a spin-wait loop. This can help improve the performance and power consumption of spin-wait loops.
```

With our original active backoff implementation, we got a performance benefit from creating a finite pause within our loop that polls (reads) the state of our spinlock. However, we got this at the cost of thousands, millions, or billions of extra instructions (depending on how long our application runs) executed for our dummy `for` loop.

We can use the `_mm_pause()` intrinsic to get this to the same effect (generating a delay between reads of our spinlock state), but without executing a huge number of extra instructions.

### The Passive Backoff Optimization

Let's take a look at our spinlock's `lock` method but with passive instead of active backoff. For this, we will be swapping out our `for` loop with calls to the `_mm_pause()` intrinsic. Here's how that may look:

```cpp
   // Locking mechanism
   void lock() {
     while (1) {
       // Try and grab the lock
       if (!locked.exchange(true)) return;
 
       do {
         // Pause for some number of iterations
         for (int i = 0; i < 4; i++) _mm_pause();
 
         // Check to see if the lock is free
       } while (locked.load());
     }
   }
```

The core functionality is exactly the same as our active backoff implementation, with the sole exception of how we generate our delays. Just like we could configure the number of iterations of our for loop to control our delay in the previous implementation, we can tune how many times we use the pause intrinsic as well.

### Low-Level Assembly

Let's take a look at how our low-level assembly for our benchmark looks with this new optimization:

```assembly
  0.34 │20:┌─→mov     %edi,%ecx                  
 17.32 │   │  xchg    %cl,(%rax)                 
  0.01 │   │  test    %cl,%cl                    
  0.01 │   │↓ je      41                         
  0.09 │   │  nop                                
 12.61 │30:│  pause                              
 12.39 │   │  pause                              
 12.32 │   │  pause                              
 12.06 │   │  pause                              
 15.37 │   │  movzbl  (%rax),%ecx                
  0.03 │   │  test    %cl,%cl                    
  0.05 │   │↑ jne     30                         
  0.42 │   │↑ jmp     20                         
  2.75 │41:│  incq    (%rsi)                     
 14.23 │   │  xchg    %cl,(%rax)                 
       │   ├──dec     %edx                       
  0.00 │   └──jne     20                        
```

No major structural changes compare to our previous implementation. We start by trying to get the lock with an atomic exchange (`xchg`). If we get the lock, we increment the shared value (`incq`), free the lock with another atomic exchange (`xchg`), decrement the loop counter (`dec`), and return to the top of the loop if there are more iterations.

If we didn't get the look, we go to our spinning locally `do while` loop with passive backoff starting at label `30:`. There, we see 4 `pause` instructions (coming from our calls to the `_mm_pause()` intrinsic), followed by our read of the spinlock state that decides whether we pause again, or try and grab the lock.

### Performance

Here are the end-to-end performance numbers for 1, 2, 4, and 8 threads:

```txt
----------------------------------------------------------------------
Benchmark                            Time             CPU   Iterations
----------------------------------------------------------------------
passive_backoff/1/real_time       1.03 ms        0.054 ms          676
passive_backoff/2/real_time       2.48 ms        0.082 ms          276
passive_backoff/4/real_time       9.59 ms        0.110 ms           70
passive_backoff/8/real_time       46.7 ms        0.236 ms           15
```

I've roughly matched the performance of our active backoff and passive backoff benchmarks. However, you may see slight differences. This is to be expected because thread scheduling is non-deterministic, and is heavily influenced by the current load of the CPUs. Therefore, we'll deem the values close enough.

### Power

To reason about the power-consumption of our applications, we'll be looking at the total number of instructions executed to compare the amount of work being done, then directly query power performance counters.

#### Instructions Executed

Let's start our power-consumption analysis by comparing the total number of instructions executed. Let's compare the 8-thread benchmark case for our active and passive passive backlock implementations with the number of benchmark iterations set to `50`.

Here are the number instructions executed for our active backoff implementation:

```txt
55,227,656,500      instructions              #    0.76  insn per cycle
```

Around 55 billion. Because thread scheduling is non-deterministic, this number varied from run to run, but generally fell between the 45-60 billion instruction range.

Now let's see how many instructions were executed for our passive backoff implementation:

```txt
1,012,803,760      instructions              #    0.01  insn per cycle         
```

A 55x reduction in instructions! But this should make sense. In our active backoff implementation, we're executing billions of extra instructions just to induce the delay inside of our locally spinning loop. In our passive backoff implementation, we're using a dedicated instruction that adds a finite delay to our pipeline, allowing us to simply stop doing work for short periods of time.

Intuitively, we can reason that our passive backoff implementation that executes 55x fewer instructions and with roughly equivilant end-to-end execution time as our active backoff should consume less power while running.

##### Additional Statistics

One thing you may notice if you look at our other performance counter stats for our passive backoff benchmark is that we have a very high branch miss-prediction and L1 data cache miss rate:

```txt
216,996,864      branches                  #   12.154 M/sec                  
 35,495,595      branch-misses             #   16.36% of all branches        
240,163,142      L1-dcache-loads           #   13.452 M/sec                  
147,908,665      L1-dcache-load-misses     #   61.59% of all L1-dcache hits  
```

What's going on? Let's think about what we know about our benchmarks and spinlock implementations. In both benchmarks, we have a heavy contention scenario where threads fight over the cache line(s) containing our spinlock state and shared value `val`. Furthermore, when a thread actually sees the lock as free or successfully gets the lock is non-deterministic, and could be difficult for our branch predictor to guess correctly. 

These things explain our observations in passive backoff, but why do these not look like a problem in our active backoff implementation?

```txt
 7,871,605,800      branches                  #  576.093 M/sec                  
    76,998,863      branch-misses             #    0.98% of all branches        
15,543,237,429      L1-dcache-loads           # 1137.551 M/sec                  
   106,735,606      L1-dcache-load-misses     #    0.69% of all L1-dcache hits  
```

Just look at the raw counter values! We're only doing ~217 million branches in our passive backoff benchmark, but ~7.9 billion in our active backoff benchmark. Likewise, our L1 data cache load increased from ~240 million to ~15.5 billion! Most of these loads and branches are from the dummy `for` loop we added to insert a delay. These are hiding the underlying randomness and lack of cache-locality that is fundamental to our application.

#### Power Performance Counters

A more direct way that we can estimate the power consumption of our benchmarks is by directly accessing the power performance counters. Specifically, we'll be looking at `power/energy-pkg/`.

Here is the estimate in power consumption for our active backoff benchmark:

```txt
-------------------------------------------------------------------------
Benchmark                               Time             CPU   Iterations
-------------------------------------------------------------------------
active_backoff/8/iterations:50       46.3 ms        0.172 ms           50

 Performance counter stats for 'system wide':

            190.59 Joules power/energy-pkg/                                           

       2.337446416 seconds time elapsed
```

And here is the estimate for our passive backoff benchmark:

```txt
--------------------------------------------------------------------------
Benchmark                                Time             CPU   Iterations
--------------------------------------------------------------------------
passive_backoff/8/iterations:50       48.7 ms        0.197 ms           50

 Performance counter stats for 'system wide':

            151.31 Joules power/energy-pkg/                                           

       2.457350713 seconds time elapsed
```

Great! Notice that even though our performance was slightly worse and our benchmark runs for longer, we're still using significantly less power (151 Joules vs. 190 Joules).

## A Spinlock with Non-Constant Backoff

### Exponential Backoff

### Random Backoff

## A Ticket Spinlock

## A pthreads Spinlock

## Final Thoughts

Thanks for reading,

--Nick

### Link to the source code

- [Source Code: ](https://github.com/CoffeeBeforeArch/spinlocks)
- [My YouTube Channel: ](https://www.youtube.com/channel/UCsi5-meDM5Q5NE93n_Ya7GA?view_as=subscriber)
- [My GitHub Account: ](https://github.com/CoffeeBeforeArch)
- My Email: CoffeeBeforeArch@gmail.com

