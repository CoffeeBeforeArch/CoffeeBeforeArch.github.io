---
layout: default
title: Spinlocks - Part 6
---

# Spinlocks - Part 6

In the [last blog post](https://coffeebeforearch.github.io/2020/11/07/spinlocks-6.html), we looked at non-constant forms of backoff that further improved the end-to-end execution time of our benchmarks. However, optimizations that help with throughput/performance may hurt other metrics like fairness.

In this blog post, we'll look at a spinlock implementation optimized for fairness called a ticket spinlock.

### Link to the source code

- [Source Code: ](https://github.com/CoffeeBeforeArch/spinlocks)
- [My YouTube Channel: ](https://www.youtube.com/channel/UCsi5-meDM5Q5NE93n_Ya7GA?view_as=subscriber)
- [My GitHub Account: ](https://github.com/CoffeeBeforeArch)
- My Email: CoffeeBeforeArch@gmail.com

## Our Benchmark

We will evaluate our spinlock implementations using [Google Benchmark](https://github.com/google/benchmark). The structure of the benchmark can be found below:

```cpp
 // Small Benchmark
 static void bench(benchmark::State &s) {
   // Sweep over a range of threads
   auto num_threads = s.range(0);
 
   // Value we will increment
   std::int64_t val = 0;
 
   // Allocate a vector of threads
   std::vector<std::thread> threads;
   threads.reserve(num_threads);
 
   Spinlock sl;
 
   // Timing loop
   for (auto _ : s) {
     for (auto i = 0u; i < num_threads; i++) {
       threads.emplace_back([&] { inc(sl, val); });
     }
     // Join threads
     for (auto &thread : threads) thread.join();
     threads.clear();
   }
 }
 BENCHMARK(bench)
     ->RangeMultiplier(2)
     ->Range(1, std::thread::hardware_concurrency())
     ->UseRealTime()
     ->Unit(benchmark::kMillisecond);
```

We'll be evaluating our benchmark for a power-of-two number of threads (1, 2, 4, and 8 on my machine), where each thread runs a function called `inc`. This function can be found below:

```cpp
 // Increment val once each time the lock is acquired
 void inc(Spinlock &s, std::int64_t &val) {
   for (int i = 0; i < 100000; i++) {
     s.lock();
     val++;
     s.unlock();
   }
 }
```

For 100k iterations, our threads try and lock our spinlock, increment a shared value, then release the lock. This gives us a heavy contention scenario in which to test our spinlock.

## A Ticket Spinlock

Before we jump into our ticket spinlock implementation, we'll discuss why we should care about things like fairness and starvation.

### Fairness and Starvation

End-to-end performance and throughput are not the only metrics to consider when optimization. In many circumstances, we also care about things like fairness and starvation.

In the case of our spinlock, starvation occurs when one thread (or a subset of threads) hog access to our spinlock, starving other one or more other threads for access. So far, we have done nothing to prevent this from happening in any of our implementations (`naive`, `spin_locally`, `active_backoff`, `passive_backoff`, `exp_backoff`, `random_backoff`).

In fact, our exponential backoff implementation actualy increases how unfair our implementation is. To understand this, let's think about the core of the `lock` method from that implementation.

```cpp
   // Locking mechanism
   void lock() {
     // Start backoff at MIN_BACKOFF iterations
     int backoff_iters = MIN_BACKOFF;
   
     // Keep trying
     while (1) {
       // Try and grab the lock
       // Return if we get the lock
       if (!locked.exchange(true)) return;
   
       // If we didn't get the lock, just read the value which gets cached
       // locally. This leads to less traffic.
       // Pause for an exponentially increasing number of iterations
       do {
         // Pause for some number of iterations
         for (int i = 0; i < backoff_iters; i++) _mm_pause();
 
         // Get the backoff iterations for next time
         backoff_iters = std::min(backoff_iters << 1, MAX_BACKOFF);
   
         // Check to see if the lock is free
       } while (locked.load());
     }
   }
```

The longer a thread waits for a spinlock, the longer it waits in the `_mm_pause()` loop. That means that a thread that just started waiting for the lock will be checking the lock state faster, and be more likely to get the lock than a thread that has been waiting for the lock for a long time, and is checking the lock state very slowly.

In order to prevent starvation and promote fairness in how access to the lock is granted, we can use a ticket-based system.

## The Ticket Spinlock Optimization

The core idea behind the ticket-spinlock is that the first thread to check if the spinlock is free should be the next thread to get the lock. What we're implementing is a FIFO (first in first out) lock.

Fundamentally, our ticket spinlock will have 4 components:

1. State for the latest ticket number
2. State for the now-serving number
3. A method for unlocking our spinlock
4. A method for locking our spinlock

### Latest Ticket Number State

for our state that maintains the latest ticket number, we're going to use a `std::atomic<std::uint16_t>`.

```cpp
   std::atomic<std::uint16_t> line{0};
```

Similar to our lock state from our previous examples, we need to make this atomic because multiple threads will be trying to get "tickets" at the same time, and trying to modify this state.

### Now Serving Number State

For our now-serving number we'll use a `volatile std::uint16_t`.

```cpp
   volatile std::uint16_t serving{0};
```

By marking this volatile, we prevent the compiler from storing this in a register. This is important, because modifying the serving number is how we signal the next thread to start working.

However, we need not make this atomic. The only thread that will be modifying the serving number is the thread currently holding the lock.

### A Lock Method

Our lock method is fairly simple:

```cpp
   // Locking mechanism
   void lock() {
     // Get the latest place in line (and increment the value)
     auto place = line.fetch_add(1);
 
     // Wait until our number is "called"
     while (serving != place)
       ;
   }
```

To lock our spinlock, we do an atomic fetch and add to get our place in line. This reads the current value of the latest ticket number state (`line`) into the `place` variable, and stores `line + 1` back to memory.

After a thread gets it's place in line, it waits for its number to be "called". This is done in the `while` loop where a thread thread waits for the `serving` number to be equal to its place in line (`place`).

Once the values are equal, the thread breaks out because it now has the lock.

### An Unlock Method

Our unlock method is still just a single line of code:

```cpp
   void unlock() { serving = serving + 1; }
```

In order to pass the lock to the next thread, we just need to increment the `serving` value by 1. Then, the thread next in line will see that its value of `serving` is equal to `place`, and break out of its lock method.


### Full Ticket Spinlock Implementation

Here is the full ticket spinlock implementation:

```cpp
 // Simple Spinlock
 // Now uses ticket system for fairness
 class Spinlock {
  private:
   // Lock is now two counters:
   //  1.) The latest place taken in line
   //  2.) Which number is currently being served
   std::atomic<std::uint16_t> line{0};
   // Needs to avoid the compiler putting this in a register!
   volatile std::uint16_t serving{0};
 
  public:
   // Locking mechanism
   void lock() {
     // Get the latest place in line (and increment the value)
     auto place = line.fetch_add(1);
 
     // Wait until our number is "called"
     while (serving != place)
       ;
   }
 
   // Unlocking mechanism
   // Increment serving number to pass the lock
   // No need for an atomic! The thread with the lock is the only one that
   // accesses this variable!
   void unlock() { serving = serving + 1; }
 };
```

## Low-Level Assembly

Let's take a look at how our low-level assembly for our benchmark looks with this new optimization:

```assembly
  0.06 │18:┌─→mov     $0x1,%edx                
 20.41 │   │  lock    xadd    %dx,(%rax)       
  0.03 │   │  nop                              
 50.39 │28:│  movzwl  0x2(%rax),%edi           
  0.05 │   │  cmp     %di,%dx                  
 24.54 │   │↑ jne     28                       
  3.58 │   │  movzwl  0x2(%rax),%edx           
  0.66 │   │  incq    (%rsi)                   
       │   │  inc     %edx                     
       │   │  mov     %dx,0x2(%rax)            
       │   ├──dec     %ecx                     
  0.27 │   └──jne     18                       
```

## Performance

Here are the end-to-end performance numbers for 1, 2, 4, and 8 threads:

```txt
------------------------------------------------------------------
Benchmark                        Time             CPU   Iterations
------------------------------------------------------------------
ticket_lock/1/real_time      0.651 ms        0.053 ms         1093
ticket_lock/2/real_time       11.3 ms        0.069 ms           67
ticket_lock/4/real_time       29.7 ms        0.103 ms           24
ticket_lock/8/real_time       69.3 ms        0.190 ms           10
```

## Final Thoughts

We've made a lot of progress in terms of performance and power consumption, but we have yet to discuss other important topics like fairness and starvation. We have no guarantee that some threads won't  be waiting for extreme amounts of time for the lock. We'll look at this in the next post with a ticket-based spinlock.

Thanks for reading,

--Nick

### Link to the source code

- [Source Code: ](https://github.com/CoffeeBeforeArch/spinlocks)
- [My YouTube Channel: ](https://www.youtube.com/channel/UCsi5-meDM5Q5NE93n_Ya7GA?view_as=subscriber)
- [My GitHub Account: ](https://github.com/CoffeeBeforeArch)
- My Email: CoffeeBeforeArch@gmail.com

