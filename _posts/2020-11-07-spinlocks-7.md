---
layout: default
title: Spinlocks - Part 7
---

# Spinlocks - Part 7

In the [last blog post](https://coffeebeforearch.github.io/2020/11/07/spinlocks-7.html), we looked at yet another implementation of a spinlock (a ticket spinlock). However, most people will never have to write their own synchronization mechanisms, and will instead use what is available to them in libraries.

In this blog post, we'll look at a pthread spinlock implementation (`pthread_spinlock_t`), and compare its performance to some of our custom spinlocks.

### Link to the source code

- [Source Code: ](https://github.com/CoffeeBeforeArch/spinlocks)
- [My YouTube Channel: ](https://www.youtube.com/channel/UCsi5-meDM5Q5NE93n_Ya7GA?view_as=subscriber)
- [My GitHub Account: ](https://github.com/CoffeeBeforeArch)
- My Email: CoffeeBeforeArch@gmail.com

## Our Benchmark

We will evaluate our spinlock implementations using [Google Benchmark](https://github.com/google/benchmark). The structure of the benchmark can be found below:

```cpp
 // Small Benchmark
 static void bench(benchmark::State &s) {
   // Sweep over a range of threads
   auto num_threads = s.range(0);
 
   // Value we will increment
   std::int64_t val = 0;
 
   // Allocate a vector of threads
   std::vector<std::thread> threads;
   threads.reserve(num_threads);
 
   // Create a spinlock
   pthread_spinlock_t sl;
   pthread_spin_init(&sl, PTHREAD_PROCESS_PRIVATE);
 
   // Timing loop
   for (auto _ : s) {
     for (auto i = 0u; i < num_threads; i++) {
       threads.emplace_back([&] { inc(sl, val); });
     }
     // Join threads
     for (auto &thread : threads) thread.join();
     threads.clear();
   }
 }
 BENCHMARK(bench)
     ->RangeMultiplier(2)
     ->Range(1, std::thread::hardware_concurrency())
     ->UseRealTime()
     ->Unit(benchmark::kMillisecond);
```

We'll be evaluating our benchmark for a power-of-two number of threads (1, 2, 4, and 8 on my machine), where each thread runs a function called `inc`. This function can be found below:

```cpp
 // Increment val once each time the lock is acquired
void inc(pthread_spinlock_t &sl, std::int64_t &val) {
  for (int i = 0; i < 100000; i++) {
    pthread_spin_lock(&sl);
    val++;
    pthread_spin_unlock(&sl);
  }
}
```

For 100k iterations, our threads try and lock the spinlock, increment a shared value, then release the lock. This gives us a heavy contention scenario in which to test the `pthread_spinlock_t`.

## The Pthread Spinlock

At this point of the post, we'd usually look at some sort of C or C++ implementation of a spinlock. Unfortunately, We have no such luxury for our `pthread_spin_lock` and `pthread_spin_unlock` functions because they are both implemented directly in assembly! Let's look at both of these implementations and try and understand how they function.

### pthread_spin_lock

Here is the implementation of `pthread_spin_lock` from glibc:

```assembly
/* Copyright (C) 2012-2020 Free Software Foundation, Inc.
   This file is part of the GNU C Library.

   The GNU C Library is free software; you can redistribute it and/or
   modify it under the terms of the GNU Lesser General Public
   License as published by the Free Software Foundation; either
   version 2.1 of the License, or (at your option) any later version.

   The GNU C Library is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   Lesser General Public License for more details.

   You should have received a copy of the GNU Lesser General Public
   License along with the GNU C Library; if not, see
   <https://www.gnu.org/licenses/>.  */

#include <lowlevellock.h>
#include <sysdep.h>

ENTRY(pthread_spin_lock)
1:	LOCK
	decl	0(%rdi)
	jne	2f
	xor	%eax, %eax
	ret

	.align	16
2:	rep
	nop
	cmpl	$0, 0(%rdi)
	jg	1b
	jmp	2b
END(pthread_spin_lock)
```

The first thing the `pthread_spin_lock` routine does is try and grab the lock using an atomic decrement (`LOCK decl`). Here, a thread gets the lock when it decrements the value at `(%rdi)` from `1` to `0` (this will set the zero flag). If the zero flag is set, the thread exits the routine, returning `0`.

If the thread does not get the lock, it jumps to a waiting loop that starts with the pair of instructions `rep` and `nop`. These instructions say to repeat a no-op for `cx` iterations. However, this pair also has the same opcode as the `pause` instruciion we introduced with our passive backoff implementation! This is a clever way to be backwards compatible. On very old processors, the instruction pair gets decoded as `rep` and `nop`, and on new processors it well be decoded as `pause`.

After our `pause` instruction (in disguise), our thread checks to see if the spinlock state is greater than `0`, and either jumps to decrement the value at `%rdi` again, or jumps to the waiting loop.

#### But Is This Safe?

One thing you might have wondered is how we can get away with decrementing the value at `(%rdi)` instead of using an atomic exchange (like we did). Won't the value eventually circle-back to `1` after enough decrements?

Yes, it will! Is this likely to happen? Certainly not. The value is only decremented when a new thread tries to grab the lock. That means we would need as many threads trying for the lock as needed to roll the value in `(%rdi)` back to 1. 100,000 threads trying for the same lock at the same time would not even be enough. We can therefore safely ignore this corner case.

### pthread_spin_unlock

Here is the implementation of `pthread_spin_unlock` from glibc:

```assembly
/* Copyright (C) 2002-2020 Free Software Foundation, Inc.
   This file is part of the GNU C Library.
   Contributed by Ulrich Drepper <drepper@redhat.com>, 2002.

   The GNU C Library is free software; you can redistribute it and/or
   modify it under the terms of the GNU Lesser General Public
   License as published by the Free Software Foundation; either
   version 2.1 of the License, or (at your option) any later version.

   The GNU C Library is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   Lesser General Public License for more details.

   You should have received a copy of the GNU Lesser General Public
   License along with the GNU C Library; if not, see
   <https://www.gnu.org/licenses/>.  */

#include <sysdep.h>

ENTRY(pthread_spin_unlock)
	movl	$1, (%rdi)
	xorl	%eax, %eax
	retq
END(pthread_spin_unlock)

	/* The implementation of pthread_spin_init is identical.  */
	.globl	pthread_spin_init
pthread_spin_init = pthread_spin_unlock
```

## Performance

Here are the end-to-end performance numbers for 1, 2, 4, and 8 threads:

```txt
```

## Final Thoughts

Ticket-based spinlocks give us a fair way for threads to busy-wait for a lock. While our end-to-end performance took a hit (compared to our exponential backoff spinlock), we were able to understand why. Throughput is not the only metric in the world, and there are plenty of situations where we want a compromise between throughput and fairness.

Thanks for reading,

--Nick

### Link to the source code

- [Source Code: ](https://github.com/CoffeeBeforeArch/spinlocks)
- [My YouTube Channel: ](https://www.youtube.com/channel/UCsi5-meDM5Q5NE93n_Ya7GA?view_as=subscriber)
- [My GitHub Account: ](https://github.com/CoffeeBeforeArch)
- My Email: CoffeeBeforeArch@gmail.com

